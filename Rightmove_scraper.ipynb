{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFCWZr9TMbNkFs9Pynbpn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamieMilliken/JamieMilliken.github.io/blob/main/Rightmove_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "6hMt11T0Kfo5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOROUGHS = {\n",
        "    \"City of London\": \"5E61224\",\n",
        "    \"Barking and Dagenham\": \"5E61400\",\n",
        "    \"Barnet\": \"5E93929\",\n",
        "    \"Bexley\": \"5E93932\",\n",
        "    \"Brent\": \"5E93935\",\n",
        "    \"Bromley\": \"5E93938\",\n",
        "    \"Camden\": \"5E93941\",\n",
        "    \"Croydon\": \"5E93944\",\n",
        "    \"Ealing\": \"5E93947\",\n",
        "    \"Enfield\": \"5E93950\",\n",
        "    \"Greenwich\": \"5E61226\",\n",
        "    \"Hackney\": \"5E93953\",\n",
        "    \"Hammersmith and Fulham\": \"5E61407\",\n",
        "    \"Haringey\": \"5E61227\",\n",
        "    \"Harrow\": \"5E93956\",\n",
        "    \"Havering\": \"5E61228\",\n",
        "    \"Hillingdon\": \"5E93959\",\n",
        "    \"Hounslow\": \"5E93962\",\n",
        "    \"Islington\": \"5E93965\",\n",
        "    \"Kensington and Chelsea\": \"5E61229\",\n",
        "    \"Kingston upon Thames\": \"5E93968\",\n",
        "    \"Lambeth\": \"5E93971\",\n",
        "    \"Lewisham\": \"5E61413\",\n",
        "    \"Merton\": \"5E61414\",\n",
        "    \"Newham\": \"5E61231\",\n",
        "    \"Redbridge\": \"5E61537\",\n",
        "    \"Richmond upon Thames\": \"5E61415\",\n",
        "    \"Southwark\": \"5E61518\",\n",
        "    \"Sutton\": \"5E93974\",\n",
        "    \"Tower Hamlets\": \"5E61417\",\n",
        "    \"Waltham Forest\": \"5E61232\",\n",
        "    \"Wandsworth\": \"5E93977\",\n",
        "    \"Westminster\": \"5E93980\",\n",
        "}"
      ],
      "metadata": {
        "id": "cpNyndHIy5Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_apartment_links = [] # stores apartment links\n",
        "all_description = [] # stores number of bedrooms in the apartment\n",
        "all_address = [] # stores address of apartment\n",
        "all_price = [] # stores the listing price of apartment"
      ],
      "metadata": {
        "id": "dFYtSWPWbK00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for borough in list(BOROUGHS.values()):\n",
        "\n",
        "        # initialise index, this tracks the page number we are on. every additional page adds 24 to the index\n",
        "        index = 0\n",
        "\n",
        "        key = [key for key, value in BOROUGHS.items() if value == borough]\n",
        "        print(f\"We are scraping the borough named: {key}\")\n",
        "        for pages in range(41):\n",
        "\n",
        "            # define our user headers\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\"\n",
        "            }\n",
        "\n",
        "            # the website changes if the you are on page 1 as compared to other pages\n",
        "            if index == 0:\n",
        "                rightmove = f\"https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%{borough}&sortType=6&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=\"\n",
        "\n",
        "            elif index != 0:\n",
        "                rightmove = f\"https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%{borough}&sortType=6&index={index}&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=\"\n",
        "\n",
        "            # request our webpage\n",
        "            res = requests.get(rightmove, headers=headers)\n",
        "\n",
        "            # check status\n",
        "            res.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "# This gets the list of apartments\n",
        "            apartments = soup.find_all(\"div\", class_=\"l-searchResult is-list\")\n",
        "\n",
        "            # This gets the number of listings\n",
        "            number_of_listings = soup.find(\n",
        "                \"span\", {\"class\": \"searchHeader-resultCount\"}\n",
        "            )\n",
        "            number_of_listings = number_of_listings.get_text()\n",
        "            number_of_listings = int(number_of_listings.replace(\",\", \"\"))\n",
        "\n",
        "            for i in range(len(apartments)):\n",
        "\n",
        "                # tracks which apartment we are on in the page\n",
        "                apartment_no = apartments[i]\n",
        "\n",
        "                # append link\n",
        "                apartment_info = apartment_no.find(\"a\", class_=\"propertyCard-link\")\n",
        "                link = \"https://www.rightmove.co.uk\" + apartment_info.attrs[\"href\"]\n",
        "                all_apartment_links.append(link)\n",
        "\n",
        "                # append address\n",
        "                address = (\n",
        "                    apartment_info.find(\"address\", class_=\"propertyCard-address\")\n",
        "                    .get_text()\n",
        "                    .strip()\n",
        "                )\n",
        "                all_address.append(address)\n",
        "\n",
        "                # append description\n",
        "                description = (\n",
        "                    apartment_info.find(\"h2\", class_=\"propertyCard-title\")\n",
        "                    .get_text()\n",
        "                    .strip()\n",
        "                )\n",
        "                all_description.append(description)\n",
        "\n",
        "                # append price\n",
        "                price = (\n",
        "                    apartment_no.find(\"div\", class_=\"propertyCard-priceValue\")\n",
        "                    .get_text()\n",
        "                    .strip()\n",
        "                )\n",
        "                all_price.append(price)\n",
        "            print(f\"You have scrapped {pages + 1} pages of apartment listings.\")\n",
        "            print(f\"You have {number_of_listings - index} listings left to go\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # code to ensure that we do not overwhelm the website\n",
        "            time.sleep(random.randint(1, 3))\n",
        "\n",
        "            # Code to count how many listings we have scrapped already.\n",
        "            index = index + 24\n",
        "\n",
        "            if index >= number_of_listings:\n",
        "                break\n",
        "                # convert data to dataframe\n",
        "data = {\n",
        "        \"Links\": all_apartment_links,\n",
        "        \"Address\": all_address,\n",
        "        \"Description\": all_description,\n",
        "        \"Price\": all_price,\n",
        "    }\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "df.to_csv(r\"sales_data.csv\", encoding=\"utf-8\", header=\"true\", index = False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "hChCLBIZbO53",
        "outputId": "6f8ca6d8-5f73-43bb-8da8-038f8f4b1263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a368a763ad47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# the website changes if you are on page 1 as compared to other pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mrightmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%{borough}&sortType=6&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'borough' is not defined"
          ]
        }
      ]
    }
  ]
}